{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    " # Table of Contents\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\" id=\"toc-level0\"><li><span><a href=\"http://localhost:8891/notebooks/Cleaning%20Data%20using%20Scikit-learn.ipynb#Count-Vectorizer\" data-toc-modified-id=\"Count-Vectorizer-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Count Vectorizer</a></span></li><li><span><a href=\"http://localhost:8891/notebooks/Cleaning%20Data%20using%20Scikit-learn.ipynb#TF-IDF-Vectorizer\" data-toc-modified-id=\"TF-IDF-Vectorizer-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>TF-IDF Vectorizer</a></span></li><li><span><a href=\"http://localhost:8891/notebooks/Cleaning%20Data%20using%20Scikit-learn.ipynb#Hashing-Vectorizer\" data-toc-modified-id=\"Hashing-Vectorizer-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Hashing Vectorizer</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "(1, 8)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[1 1 1 1 1 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "def explore_count_vectorizer():\n",
    "    text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(text)\n",
    "    \n",
    "    print(vectorizer.vocabulary_)\n",
    "    vector = vectorizer.transform(text)\n",
    "    print(vector.shape)\n",
    "    print(type(vector))\n",
    "    print(vector.toarray())\n",
    "    \n",
    "explore_count_vectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "[ 1.69314718  1.28768207  1.28768207  1.69314718  1.69314718  1.69314718\n",
      "  1.69314718  1.        ]\n",
      "(1, 8)\n",
      "[[ 0.36388646  0.27674503  0.27674503  0.36388646  0.36388646  0.36388646\n",
      "   0.36388646  0.42983441]]\n"
     ]
    }
   ],
   "source": [
    "def explore_tfidf():\n",
    "    text = [\"The quick brown fox jumped over the lazy dog.\",\n",
    "            \"The dog.\",\n",
    "            \"The fox\"]\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(text)\n",
    "    \n",
    "    print(vectorizer.vocabulary_)\n",
    "    print(vectorizer.idf_)\n",
    "\n",
    "    vector = vectorizer.transform([text[0]])\n",
    "    print(vector.shape)\n",
    "    print(vector.toarray())\n",
    "    \n",
    "explore_tfidf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashing Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20)\n",
      "[[ 0.          0.          0.          0.          0.          0.33333333\n",
      "   0.         -0.33333333  0.33333333  0.          0.          0.33333333\n",
      "   0.          0.          0.         -0.33333333  0.          0.\n",
      "  -0.66666667  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "def explore_hashing():\n",
    "    text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "    vectorizer = HashingVectorizer(n_features=20)\n",
    "    \n",
    "    vector = vectorizer.transform(text)\n",
    "    print(vector.shape)\n",
    "    print(vector.toarray())\n",
    "    \n",
    "explore_hashing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
